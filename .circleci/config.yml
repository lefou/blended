version: 2
jobs:
  # -----------------------------------------------------------------------------------------------
  # Just prepare the build by running sbt update - this should throw up if any dependencies cannot
  # be resolved.
  # -----------------------------------------------------------------------------------------------
  prepare:
    shell: /bin/sh -leo pipefail
    working_directory: /home/blended/blended
    docker:
      - image: atooni/build-alpine:1.0
        user: blended
    steps:
      - checkout

      # When restoring the cache we omit the {{ epoch }}, so that the most recent one will be used
      - restore-cache:
          keys:
            - v3-blended-core-{{ arch }}-{{ checksum "build.sbt" }}

      - run: |
          sbt clean update

      # Using the epoch as part od the caching key enforces a new cache to be generated even if an old one exists
      - save_cache:
          key: v3-blended-core-{{ arch }}-{{ checksum "build.sbt" }}
          paths:
            - /home/blended/.ivy2

  # -----------------------------------------------------------------------------------------------
  # perform the actual compile job. Here we simply run sbt publish local, so that all our modules
  # end up in the local ivy directory
  # -----------------------------------------------------------------------------------------------
  compile:
    shell: /bin/sh -leo pipefail
    working_directory: /home/blended/blended
    docker:
      - image: atooni/build-alpine:1.0
        user: blended
    steps:
      - checkout

      - restore-cache:
          keys:
            - v3-blended-core-{{ arch }}-{{ checksum "build.sbt" }}

      - run:
          name: Compile
          command: sbt clean publishLocal

      - run:
          name: package_ivy_local
          command: tar -C /home/blended/.ivy2 -cvzf /home/blended/blended/ivylocal.tgz local

      - persist_to_workspace:
          root: /home/blended/blended
          paths:
            - ivylocal.tgz

  # -----------------------------------------------------------------------------------------------
  # Run all unit tests, produce a coverage report and upload coverage data to codacy
  # we will attach the coverage data and other things to the current workspace, so that
  # we can use it in later jobs within the workflow
  # -----------------------------------------------------------------------------------------------
  unit-test:
    shell: /bin/sh -leo pipefail
    working_directory: /home/blended/blended
    docker:
      - image: atooni/build-alpine:1.0
        user: blended
    steps:
      - checkout

      - restore-cache:
          keys:
            - v3-blended-core-{{ arch }}-{{ checksum "build.sbt" }}

      - run:
          name: Coverage
          command: |
            bash /home/blended/blended/scripts/batchTest.sh
          environment:
            - TEST_ONLINE: true
            - SBT_OPTS: -Xmx1024M -XX:MaxMetaspaceSize=512m

      - run:
          name: Coverage Report
          command: sbt coverageReport coverageAggregate codacyCoverage

      - run:
          name: Save test results
          command: |
            mkdir -p /home/blended/blended/target/test-results/junit/
            find /home/blended/blended -type f -regex ".*/test-reports/.*xml" -exec cp {} /home/blended/blended/target/test-results/junit/ \;
            xunit-viewer --results=/home/blended/blended/target/test-results/junit --output=/home/blended/blended/target/test-results/junit.html
          when: always

      - run:
          name: Build docs
          command: sbt unidoc jbakeSite

      - store_test_results:
          path: /home/blended/blended/target/test-results

      - store_artifacts:
          path: /home/blended/blended/target/test-results
          prefix: XUnit

      - store_artifacts:
          path: /home/blended/blended/target/scala-2.12/scoverage-report
          prefix: coverage

      - store_artifacts:
          path: /home/blended/blended/target/testlog
          prefix: testlog

      # The normal persist workspace does not support to add directories recursively, so we create an
      # archive and persist that. Later jobs have to unwrap the tar file
      - run:
          name: package_workspace
          command: tar -C /home/blended/blended -cvzf /home/blended/blended/target.tgz target

      - persist_to_workspace:
          root: /home/blended/blended
          paths:
            - target.tgz

workflows:
  version: 2
  blended:
    jobs:
      - prepare

      - compile:
          requires:
            - prepare
      - unit-test:
          context: blended
          requires:
            - compile

